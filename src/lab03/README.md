# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3
## –ó–∞–¥–∞–Ω–∏–µ A (text.py)
```python
import re

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    #–§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –µ–¥–∏–Ω–æ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Å—Ç–∏–ª—é
    s=text
    if casefold :
        s=s.casefold()
    if yo2e :
        s=s.replace("—ë","–µ").replace("–Å","–ï")
    s=s.replace("\t"," ").replace("\r"," ").replace("\n"," ")
    s = ' '.join(s.split())
    s=s.strip()

    return s

def tokenize(text: str) -> list[str]:
    #–§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ ¬´—Å–ª–æ–≤–∞¬ª –ø–æ –Ω–µ–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
    pattern= r'\w+(?:-\w+)*'
    tokenstext = re.findall(pattern, text)

    return tokenstext

def count_freq(tokens: list[str]) -> dict[str, int]:
    #–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–æ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    counts={}
    for word in tokens:
        counts[word]=counts.get(word,0)+1
    return counts

def sort_key(item):
    return [-item[1], item[0]]

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    #–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã
    sorted_freq= sorted(freq.items(),key=sort_key)
    top_n=[]

    for i in range(min(n, len(sorted_freq))):
        top_n.append((sorted_freq[i][0], sorted_freq[i][1]))

    return top_n

def summary(text):
    normalized_text = normalize(text)

    tokens = tokenize(normalized_text)

    total_words = len(tokens)
    freq_sorted = count_freq(tokens)
    unique_words = len(freq_sorted)
    top = top_n(freq_sorted, 5)

    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
    print("–¢–æ–ø-5:")

    for word, count in top:
        print(f"{word}:{count}")
```
–°–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞—è —Å–ª–æ–≤–∞ –∏ –≤—ã–≤–æ–¥—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏.

–î–ª—è —Ç–µ—Å—Ç–∞ —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞, —è –Ω–∞–ø–∏—Å–∞–ª –≤—Ç–æ—Ä–æ–π —Å–∫—Ä–∏–ø—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤—ã–∑–æ–≤ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ (test_case.py)
```python
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –ø–æ–∏—Å–∫–∞ –º–æ–¥—É–ª–µ–π
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from src.lib.text import *

print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e=True))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))

print(top_n(count_freq(["a", "b", "a", "c", "b", "a"]), n=2))
print(top_n(count_freq(["bb", "aa", "bb", "aa", "cc"]), n=2))
```
–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞, –ø–æ–ª—É—á–∞–µ–º –≤–æ—Ç —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

[![401.png](https://i.postimg.cc/ZnJCPzsD/401.png)](https://postimg.cc/S2tSq3hW)

## –ó–∞–¥–∞–Ω–∏–µ B (text_stats.py)

```python
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from src.lib.text import normalize, tokenize, count_freq, top_n

def main():
    # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å –≤—Ö–æ–¥ –¥–æ EOF
    input_text = sys.stdin.readline()


    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    text_norm = normalize(input_text)
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = tokenize(text_norm)
    # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç
    freq = count_freq(tokens)

    # –ü–æ–¥—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    words_total = len(tokens)
    unique_words = len(freq)

    # –¢–æ–ø 5 —Å–ª–æ–≤
    top_words = top_n(freq, 5)

    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {words_total}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
    print("–¢–æ–ø-5:")
    for word, count in top_words:
        print(f"{word}:{count}")

if __name__ == "__main__":
    main()
```
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ stdin –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º.
–ó–∞–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç, –æ–Ω –æ–∂–∏–¥–∞–µ—Ç –≤–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–∞—Å, –º—ã –≤–≤–æ–¥–∏–º "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!", –∏ –ø–æ–ª—É—á–∞–µ–º –≤–æ—Ç —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

[![402.png](https://i.postimg.cc/x878wN1T/402.png)](https://postimg.cc/kRQ784Sk)

